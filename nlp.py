import re
import spacy
import nltk
import torch

from translate import Translator
from transformers import pipeline
from transformers import BertTokenizer, BertModel

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk import FreqDist

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from langdetect import detect
from textblob import TextBlob

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')

class Tasks:
    @staticmethod
    def word_frequency(text, key):
        words = word_tokenize(text.lower())
        return FreqDist(words)[key.lower()]
    # end define
    
    @staticmethod
    def language(text):
        return detect(text)
    # end define

    @staticmethod
    def sentance_compare(text1, text2):
        vectors = CountVectorizer().fit_transform([text1, text2])
        similarity_matrix = cosine_similarity(vectors)
        return similarity_matrix[0, 1] * 100
    # end define

    @staticmethod
    def analyze_sentiment(text):
        sentiment = TextBlob(text).sentiment.polarity
        if sentiment > 0: return "PozitÄ«vs"
        elif sentiment < 0: return "NegatÄ«vs"
        else: return "NeitrÄls"
    # end define

    @staticmethod
    def normalize(text):
        text = re.sub(r"http\S+", "", text)
        text = re.sub(r'@\w+', '', text)
        text = re.sub(r'[!?.]+', '', text)
        text = re.sub(r'[^\w\s]', '', text)
        text = re.sub(r'\s+', ' ', text).strip().lower()        
        return text
    # end define

    @staticmethod
    def summarize(text):
        sentences = sent_tokenize(text)
        stop_words = set(stopwords.words('english'))
        words = [word.lower() for word in word_tokenize(text) if word.isalnum() and word.lower() not in stop_words]
        word_frequencies = FreqDist(words)
        sentence_scores = {sentence: sum(word_frequencies[word.lower()] for word in word_tokenize(sentence) if word.lower() in word_frequencies) for sentence in sentences}
        summarized_sentences = sorted(sentence_scores, key=sentence_scores.get, reverse=True)[:2]
        return " ".join(summarized_sentences)
    # end define

    @staticmethod
    def word_embeddings(word1, word2, word3):
        model = BertModel.from_pretrained('bert-base-multilingual-cased')
        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

        def get_embedding(word):
            inputs = tokenizer(word, return_tensors='pt', truncation=True, padding=True, max_length=128)
            with torch.no_grad():
                return model(**inputs).last_hidden_state[0, 0, :].numpy()

        embeddings = {word: get_embedding(word) for word in [word1, word2, word3]}

        similarities = {
            (word1, word2): cosine_similarity([embeddings[word1]], [embeddings[word2]])[0][0],
            (word1, word3): cosine_similarity([embeddings[word1]], [embeddings[word3]])[0][0],
            (word2, word3): cosine_similarity([embeddings[word2]], [embeddings[word3]])[0][0],
        }

        for (w1, w2), sim in similarities.items():
            print(f"Similarity between '{w1}' and '{w2}': {sim:.4f}")

        return
    # end define

    @staticmethod
    def recognize_phrases(text):
        nlp = spacy.load("xx_ent_wiki_sm")
        doc = nlp(text)

        # modelis sÅ«dÄ«gi saprot latvieÅ¡u valodu
        for ent in doc.ents:
            if ent.label_ == "PERSON":
                print(f"PersonvÄrds: {ent.text}")
            elif ent.label_ == "ORG":
                print(f"OrganizÄcija: {ent.text}")
    #end define

    @staticmethod
    def generate_text(text):
        generator = pipeline("text-generation", model="gpt2")
        text = Translator(from_lang="lv", to_lang="en").translate(text)
        json = generator(
            text,
            max_length=60,
            num_return_sequences=1,
            temperature=1.2,
            truncation=True
        )

        result = Translator(from_lang="en", to_lang="lv").translate(json[0]["generated_text"]) 
        return print(result)
    # end defince

    @staticmethod
    def translate_text(text):
        text = Translator(from_lang="lv", to_lang="en").translate(text)
        return print(text)
    # end define
# end class


class Examples():
    @staticmethod
    def get_frequency():
        print("1.usd\n")
        text = "MÄkoÅ†ainÄ dienÄ kaÄ·is sÄ“dÄ“ja uz palodzes. KaÄ·is domÄja, kÄpÄ“c debesis ir pelÄ“kas. KaÄ·is gribÄ“ja redzÄ“t sauli, bet saule slÄ“pÄs aiz mÄkoÅ†iem."
        key = "KaÄ·is"

        return print(f"Frequency of key {key}: {Tasks.word_frequency(text, key)}")
    # end define

    @staticmethod
    def get_language():
        print("2.usd\n")
        texts = [
            "Å odien ir saulaina diena.",
            "Today is a sunny day.",
            "Ğ¡ĞµĞ³Ğ¾Ğ´Ğ½Ñ ÑĞ¾Ğ»Ğ½ĞµÑ‡Ğ½Ñ‹Ğ¹ Ğ´ĞµĞ½ÑŒ.",
        ]

        for text in texts:
            print(Tasks.language(text))
    # end define

    @staticmethod
    def get_sentance_compare():
        print("3.usd\n")
        text1 = "Rudens lapas ir dzeltenas un oranÅ¾as. Lapas klÄj zemi un padara to krÄsainu."
        text2 = "KrÄsainas rudens lapas krÄ«t zemÄ“. Lapas ir oranÅ¾as un dzeltenas."

        return print(f"VÄrdu sakritÄ«bas lÄ«menis: {Tasks.sentance_compare(text1, text2):.2f}%")
    # end define

    @staticmethod
    def get_analyze_sentiment():
        print("4.usd\n")
        sentences = [
            "Å is produkts ir lielisks, esmu Ä¼oti apmierinÄts!",
            "Esmu vÄ«lies, produkts neatbilst aprakstam.",
            "NeitrÄls produkts, nekas Ä«paÅ¡s."
        ]

        for sentence in sentences:
            sentiment = Tasks.analyze_sentiment(sentence)
            print(f"Teikums: \"{sentence}\" - NoskaÅ†ojums: {sentiment}")
    # end define

    @staticmethod
    def get_normalize():
        print("5.usd\n")
        text = "@John: Å is ir lielisks produkts!!! Vai ne? ğŸ‘ğŸ‘ğŸ‘ http://example.com"

        return print(Tasks.normalize(text))
    # end define

    @staticmethod
    def get_summarize():
        print("6.usd\n")
        text = """
            Latvija ir valsts Baltijas reÄ£ionÄ. TÄs galvaspilsÄ“ta ir RÄ«ga, kas ir slavena ar savu vÄ“sturisko centru un skaistajÄm Ä“kÄm. 
            Latvija robeÅ¾ojas ar Lietuvu, Igauniju un Krieviju, kÄ arÄ« tai ir piekÄ¼uve Baltijas jÅ«rai. 
            TÄ ir viena no Eiropas SavienÄ«bas dalÄ«bvalstÄ«m.
            """
        
        return print(Tasks.summarize(text))
    # end define

    @staticmethod
    def get_word_embeddings():
        print("7.usd\n")

        return Tasks.word_embeddings("mÄja", "dzÄ«voklis", "jÅ«ra")
    # end define

    @staticmethod
    def get_recognize_phrases():
        print("8.usd\n")

        return Tasks.recognize_phrases("Valsts prezidents Egils Levits piedalÄ«jÄs pasÄkumÄ, ko organizÄ“ja Latvijas UniversitÄte.")
    # end define

    @staticmethod
    def get_generate_text():
        print("9.usd\n")

        return Tasks.generate_text("Reiz kÄdÄ tÄlÄ zemÄ“...")
    # end define

    @staticmethod
    def get_translate():
        print("10.usd\n")

        texts = [
            "Labdien! KÄ jums klÄjas?",
            "Es Å¡odien lasÄ«ju interesantu grÄmatu.",
        ]

        for text in texts:
            Tasks.translate_text(text)
    # end define
# end class

Examples.get_frequency()
Examples.get_language()
Examples.get_sentance_compare()
Examples.get_analyze_sentiment()
Examples.get_normalize()
Examples.get_summarize()
Examples.get_word_embeddings()
Examples.get_recognize_phrases()
Examples.get_generate_text()
Examples.get_translate()
